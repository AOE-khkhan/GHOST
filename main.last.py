# -*- coding: utf-8 -*-
"""GHOST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iMcdaYlNYcGoQJLCharUtCE7Ev2AbfYI
"""
# from numba import jit

def toBin(c, n=8):
    if type(c) == str:
        c = ord(c)
        
    b = bin(c)[2:]
    return ''.join(['0' for _ in range(n - len(b))]) + b

def toChar(b):
    n = int('0b{}'.format(b), 2)
    return n.to_bytes((n.bit_length() + 7) // 8, 'big').decode()

def train(filepath):
    with open(filepath, 'r') as f:
        lines = f.readlines()
        for line in lines:
            for s in list(line.strip()):
                yield s
            yield '`'

def learn_counting(n=101, n_iter=1):
    for _ in range(n_iter):
        print('\nthis is iteration {} of {} iteration(s): counting to {}\n'.format(_+1, n_iter, n))
        for i in range(n):
            data = str(i)
            for c in list(data):
                yield c
            yield '`'

def log(output='', title=None):
    if type(output) in [str, int, type(None)]:
        print('{} = {}'.format(title, output))
        return

    if title != None:
        print('\n{} \n{}'.format(title, ''.join(['=' for _ in range(len(title)+2)])))

    print('{}\n'.format(output))
    return


'''
Author: Joshua, Christian r0b0tx
Date: 1 Nov 2018
File: processor.py
Project: GHOST
'''

# import from python standard lib
from itertools import combinations

class Processor:
    """docstring for Processor"""    
    
    def __init__(self, state_size=8, size=3):
        # if to show output
        self.log_state = True

        # processor size in bits
        self.SIZE = size
        self.CONCEPT_SIZE = sum([2**x for x in range(self.SIZE)])

        # the number of methods used: pdf and transformations
        self.N_METHODS = 2

        # sensory binary data size
        self.STATE_SIZE = 2**state_size

        # matrix of nodes that process and project data
        self.nodes = []
        self.node_state_freq = []

        # holds the last information of size length
        self.context = []

        # concepts weights
        self.node_weight_pdf = []
        self.node_weight_transformation = []

        # holds the last concepts
        self.last_concepts = []

        # holds the hidden relation between states and concepts
        self.node_transformations = []
        self.node_transformation_total_freq = []
        self.node_transformation_freq = []

        # used to model
        self.transformations_used = []
        self.pdf_used = []

        for _ in range(self.STATE_SIZE):
            self.pdf_used.append([])
            self.transformations_used.append([])
        
        # for dynamic programming
        self.memoized = {}
        self.MEMOIZED_SIZE = 256

    def addNode(self, concept_model):
        # add concepts
        self.nodes.append(concept_model)

        # matrix of concpts to states
        self.node_state_freq.append([0.0 for _ in range(self.STATE_SIZE)])
        
        # matrix of concepts to transformations        
        self.node_transformations.append([])
        self.node_transformation_freq.append([])
        self.node_transformation_total_freq.append([]) # using this cos transformations are not necessarily mutually exclusive

        # concept weights
        self.node_weight_pdf.append([0.0 for _ in range(2)])
        self.node_weight_transformation.append([0.0 for _ in range(2)])

    def addToContext(self, data):
        if len(self.context) == self.SIZE:
            self.context.pop()
        
        self.context.insert(0, data)
        return

    def getMaxProbabilityStates(self, data):
        ret = self.getMemoized('getMaxProbabilityStates', (data))
        if ret != None:
            return ret

        mps = {}
        for concept_index, concept in enumerate(self.nodes):
            states = self.node_state_freq[concept_index]
            max_probability = max(states)
            max_probability_states = [ix for ix, x in enumerate(states) if x == max_probability]
            if data in max_probability_states:
                mps[concept] = max_probability_states

        self.updateMemoized('getMaxProbabilityStates', (data), mps)
        return mps

    def getConcepts(self, context=None, size=None, reverse=False):
        if context == None:
            context = self.context.copy()
            size = self.SIZE

        if reverse and len(context) < size:
            return []

        val = '0' if reverse else '1'
        concepts = []

        # get concept range based on precent concept as the context might not be up to max
        concept_range = sum([2**x for x in range(len(context))])
        m = size - 1
        for n in range(concept_range):
            bin_li = list(toBin(n+1, size))
            concept = [context[m - i] for i, x in enumerate(bin_li) if x == val]
            if reverse:
                concept.reverse()
            concept = tuple(concept)
            concepts.append(concept)
        return concepts

    def getConceptScoreWeight(self, scores):
        scores_ = scores.copy()
        if type(scores) == dict:
            scores_ = [value for value in scores.values()]
        concept_ranks = self.getRanks(scores_)
        if type(scores) == dict:
            return {score:concept_ranks[scores_.index(scores[score])]/self.CONCEPT_SIZE for score in scores}

        else:
            return [rank/self.CONCEPT_SIZE for rank in concept_ranks]

    def getDataIndices(self, data, dataList):
        return [index+1 for index, value in enumerate(dataList) if value == data]

    def getRanks(self, dataList):
        sortedDataList = sorted(dataList)
        ranks = []
        ranksFound = {}
        for index, value in enumerate(dataList):
            if value in ranksFound:
                rank = ranksFound[value]

            elif dataList.count(value) > 1:
                indices = self.getDataIndices(value, sortedDataList)
                rank = self.mean(indices)

            else:
                rank = sortedDataList.index(value) + 1

            ranksFound[value] = rank
            ranks.append(rank)
        return ranks

    def getMemoized(self, function_name, arguments):
        if function_name not in self.memoized:
            self.memoized[function_name] = {}
            
        if arguments not in self.memoized[function_name]:
            return
        return self.memoized[function_name][arguments]
    
    def getMaxValueIndices(self, processes, return_max_weight=False):
        m = max(processes) if len(processes) > 0 else 0
        predicted_outputs = [state for state, x in enumerate(processes) if x == m]
        if return_max_weight:
            return predicted_outputs, m

        else:
            return predicted_outputs

    def getTransformations(self, concept_x, concept_y):
        concepts = [concept_y]
        for ix, cx in enumerate(concept_x):
            new_concepts = []
            for concept in concepts:
                for iy, cy in enumerate(concept):
                    if type(cy) == str:
                        continue
                    
                    if cx == cy:
                        c_new = list(concept)
                        c_new[iy] = str(ix)
                        new_concepts.append(tuple(c_new))
            concepts = new_concepts.copy()
        return new_concepts
        
    def log(self, output, title=None):
        if not self.log_state:
            return

        log(output, title)
        return

    def mean(self, li):
        if len(li) > 0:
            return sum(li)/len(li)

        else:
            return 0

    def normalize(self, li):
        s = sum(li) if type(li) != dict else sum(li.values())
        if s > 0:
            if type(li) == dict:
                return {x:li[x]/s for x in li}

            else:
                return [x/s for x in li]

        else:
            if type(li) == dict:
                return {x:0 for x in li}

            else:
                return [0 for _ in li]

    # @jit(nopython=True) # Set "nopython" mode for best performance, equivalent to @njit
    def process(self, data):
        if len(self.context) < self.SIZE:
            self.addToContext(data)
            return

        # update the node network
        self.update(data)

        # add data to context
        self.addToContext(data)

# -------------------------------------------the process instance-------------------------------------
        # the  construct for the procesess
        pdf_processes, transformation_processes = [], []

        # the session variables
        self.transformations_used, self.pdf_used = [], []

        # index of the concepts
        concept_node_indices = []

        for _ in range(self.STATE_SIZE):
            pdf_processes.append([])
            transformation_processes.append([])

            self.pdf_used.append([])
            self.transformations_used.append([])

        # get the states at this instance
        concepts = self.getConcepts()

        for concept_index, concept in enumerate(concepts):
            concept_model = (concept_index, concept)

            if concept_model not in self.nodes:
                self.addNode(concept_model)

            # the id of the concept
            concept_node_index = self.nodes.index(concept_model)

            # save indices
            concept_node_indices.append(concept_node_index)

            # the max states:Purpose is to avoid noise
            concept2states_freq, state_weight = self.getMaxValueIndices(self.normalize(self.node_state_freq[concept_node_index]), True)
                
            # the concept weights            
            node_freq, node_total_freq = self.node_weight_pdf[concept_node_index]
            pdf_node_weight = node_freq / node_total_freq if node_total_freq > 0 else 0

            # update the concept weight
            self.node_weight_pdf[concept_node_index][1] += 1
# ------------------------------------------probability density function---------------------------------------
            for state in concept2states_freq:
                # calculate the total weight of the concept
                weight = state_weight * pdf_node_weight

                # include teh weight in the processes
                pdf_processes[state].append(weight)

                # add the concpts used in the pdf process
                self.pdf_used[state].append(concept_node_index)

                # if len(self.context) > 2 and self.context[-2] == 57 and self.context[-3] == 96 and weight > 0:
                #     print('cm = {}, state = {}, pdf_concept_weight = {} / {} = {} * {} = {}'.format(concept_model, state, node_freq, node_total_freq, pdf_node_weight, state_weight, weight))

# --------------------------------------------------transformations---------------------------------------------------
            transformations = self.node_transformations[concept_node_index]
            tf = self.node_transformation_freq[concept_node_index]      #transformation freq
            ttf = self.node_transformation_total_freq[concept_node_index] #transformation total freq

            # get the transformation_weights
            transformation_weights = [tf[i] / ttf[i] if ttf[i] > 0 else 0 for i in range(len(transformations))]

            max_transformation_weight_ids, transformation_weight = self.getMaxValueIndices(transformation_weights, True)

# ==============================================back to transformations===========================
            for transformation_index in max_transformation_weight_ids:

                # the transformation model
                transformation_model = transformations[transformation_index]

                # the tarsnformation model index
                transformation_model_id = (concept_node_index, transformation_index)

                # the ids for the transformations
                concept_x_index, transformation, concept_y_index = transformation_model

                # transform the transformation
                concept_transform = self.solveTransformation(transformation, concepts[concept_x_index])

                concept_transform_model = (concept_y_index, concept_transform)
                if concept_transform_model not in self.nodes:
                    continue

                # concept transf index
                concept_transform_node_index = self.nodes.index(concept_transform_model)

                # the max states:Purpose is to avoid noise
                concept2states_freq, state_weight = self.getMaxValueIndices(self.normalize(self.node_state_freq[concept_transform_node_index]), True)
                
                for state in concept2states_freq:
                    weight = self.trustFactor(tf[transformation_index]) * transformation_weight
                    if self.context[-1] == 53 and self.context[-2] == 96 and concept_transform == (96, 53, 96) and concept_model == (5, (53,96)):
                        print('concept = {}-{}, transf_weight = {} * {} = {}, ct = {}, s= {}-{}'.format(
                            concept_index, concepts[concept_index], format(transformation_weight, '.3f'), format(self.trustFactor(tf[transformation_index]), '.3f'), format(weight, '.3f'), concept_transform_model, transformation_model, state
                            )
                        )
                    
                    # save the weights
                    transformation_processes[state].append(weight)

                    # to hold the transformation indices of the transformations used
                    self.transformations_used[state].append(transformation_model_id)

                # increment the total no of outcomes of transformation used
                ttf[transformation_index] += 1

# =========================influence the pdf and transformations with the concept weights============
        # pdf
        self.pdf_used = [[self.pdf_used[state][i] for i, xx in enumerate(x) if len(x) > 0 and xx == max(x)] for state, x in enumerate(pdf_processes)]
        pdf_processes = [max(x) if len(x) > 0 else 0 for x in pdf_processes]

        # transformation
        self.transformations_used = [[self.transformations_used[state][i] for i, xx in enumerate(x) if len(x) > 0 and xx == max(x)] for state, x in enumerate(transformation_processes)]
        # transformation_processes = [max(x) if len(x) > 0 else 0 for x in transformation_processes]
        processes = [max(x) if len(x) > 0 else 0 for x in transformation_processes]

        # THE COMBINATION OF PDF AND TRANSFORMATION
        # processes = [max([pdf_processes[i], transformation_processes[i]]) for i in range(self.STATE_SIZE)]

        # get the max vals
        predicted_outputs, max_weight = self.getMaxValueIndices(processes, True)
        
        self.last_concepts = concepts.copy()
        self.last_concept_indices = concept_node_indices.copy()
        po = []
        return predicted_outputs, max_weight, po

    def resetMemoized(self, function_name):
        if function_name not in self.memoized:
            self.memoized[function_name] = {}
            return

        self.memoized.pop(function_name)
        self.memoized[function_name] = {}
        return 
            
    def solveTransformation(self, transformation, concept):
        transform = list(transformation)
        for i, t in enumerate(transformation):
            if type(t) != str:
                continue

            index = int(t)
            transform[i] = concept[index]
        return tuple(transform)

    def trustFactor(self, x):
        if type(x) == list:
            x = len(x)
        return x / (1 + x) 

    def update(self, data):
        self.resetMemoized('getMaxProbabilityStates')
        max_probability_states = self.getMaxProbabilityStates(data)    

        # the model concepts used
        transformations_used = self.transformations_used[data]
        pdf_used = self.pdf_used[data]

# ======================================increment the transformation================================
        for transformation_model_id in transformations_used:
            concept_node_index, transformation_index = transformation_model_id
            self.node_transformation_freq[concept_node_index][transformation_index] += 1
                    
# ======================================updating pdf, pdf_concept and creating transformations===============        
        for concept_index, concept in enumerate(self.last_concepts):
            concept_model = (concept_index, concept)
            concept_node_index = self.nodes.index(concept_model)
            
# =====================================increment the node for pdf===============================
            self.node_state_freq[concept_node_index][data] += 1

# =========================================update the pdf concepts for ranking======================
            if concept_node_index in pdf_used:
                self.node_weight_pdf[concept_node_index][0] += 1

# ======================================create transformation weights=========================================
            for concept_model in max_probability_states:
                level, other_concept = concept_model
                factor = len(max_probability_states[concept_model])**-1

                if factor < 1:
                    continue

                if not all([x in other_concept for x in concept]):
                    continue

                transformations = self.getTransformations(concept, other_concept)
                for transformation in transformations:
                    transformation_model = (concept_index, transformation, level)

                    for cni in self.last_concept_indices:
                        if transformation_model in self.node_transformations[cni]:
                            continue

                        if self.context[-1] == 53 and self.context[-2] == 96 and other_concept==(96,53,96) and self.nodes[cni] == (5, (53,96)):
                            print(self.nodes[cni], other_concept)
                        self.node_transformations[cni].append(transformation_model)
                        self.node_transformation_freq[cni].append(0)
                        self.node_transformation_total_freq[cni].append(0)
        return

    def updateMemoized(self, function_name, arguments, value):
        if function_name not in self.memoized:
            self.memoized[function_name] = {}

        vals = self.memoized[function_name]
        length = len(vals)
        if length == self.MEMOIZED_SIZE:
            self.memoized[function_name].pop([x for x in vals.keys()][random.randint(length)])

        self.memoized[function_name][arguments] = value
        return

'''
Author: Joshua, Christian r0b0tx
Date: 1 Nov 2018
File: main.py
Project: GHOST
'''

# initialize objects
PROCESSOR = Processor()

def main():
    # td = [learn_counting(11, 3), learn_counting(14, 1)]
    td = [learn_counting(11, 3), learn_counting(21, 2), learn_counting(31, 1)]#, learn_counting(41, 1), learn_counting(51, 1)]#, learn_counting(61, 2), learn_counting(71, 1), learn_counting(101, 1)]
    # td = [learn_counting(11, 3), learn_counting(21, 3), learn_counting(31, 2), learn_counting(51, 2), learn_counting(61), learn_counting(101)]
    # td = [learn_counting(21), learn_counting(11), train('train.old.txt'), learn_counting(11), train('train.old.txt')]

    # initialize
    last_outputs = []
    last_input_data = None
    weight = 0
    po = None
    
    for training_data in td:
        for c in training_data:
            if weight == 0 or len(last_outputs) > 9:
                last_outputs = ['a lot']
            print('x = {}, y = {}, y_pred = {}, weight = {}-{}, '.format(last_input_data, c, last_outputs, weight, po))

            data = ord(c)

            result = PROCESSOR.process(data)
            if result == None:
                outputs, weight, po = None, None, None

            else:
                outputs, weight, po = result
                outputs = [str(chr(x)).encode('utf-8') for x in outputs]
            
                last_outputs = outputs.copy()
            last_input_data = c
if __name__ == '__main__':
    main()

