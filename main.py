# -*- coding: utf-8 -*-
"""GHOST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iMcdaYlNYcGoQJLCharUtCE7Ev2AbfYI
"""
# from numba import jit

def toBin(c):
    if type(c) == str:
        c = ord(c)
        
    b = bin(c)[2:]
    return ''.join(['0' for _ in range(8 - len(b))]) + b

def toChar(b):
    n = int('0b{}'.format(b), 2)
    return n.to_bytes((n.bit_length() + 7) // 8, 'big').decode()

def train(filepath):
    with open(filepath, 'r') as f:
        lines = f.readlines()
        for line in lines:
            for s in list(line.strip()):
                yield s
            yield '`'

def learn_counting(n=101, n_iter=1):
    for _ in range(n_iter):
        print('\nthis is iteration {} of {} iteration(s): counting to {}\n'.format(_+1, n_iter, n))
        for i in range(n):
            data = str(i)
            for c in list(data):
                yield c
            yield '`'

def log(output='', title=None):
    if type(output) in [str, int, type(None)]:
        print('{} = {}'.format(title, output))
        return

    if title != None:
        print('\n{} \n{}'.format(title, ''.join(['=' for _ in range(len(title)+2)])))

    print('{}\n'.format(output))
    return


'''
Author: Joshua, Christian r0b0tx
Date: 1 Nov 2018
File: processor.py
Project: GHOST
'''

# import from python standard lib
from itertools import combinations

class Processor:
    """docstring for Processor"""    
    
    def __init__(self, state_size=8, size=3):
        # if to show output
        self.log_state = True

        # processor size in bits
        self.SIZE = size
        self.CONCEPT_SIZE = sum([2**x for x in range(self.SIZE)])

        # the number of methods used: pdf and transformations
        self.N_METHODS = 2

        # sensory binary data size
        self.STATE_SIZE = 2**state_size

        # matrix of nodes that process and project data
        self.nodes = [[] for _ in range(self.CONCEPT_SIZE)]
        self.node_freq = [[] for _ in range(self.CONCEPT_SIZE)]
        self.totals = [[] for _ in range(self.CONCEPT_SIZE)] #containing: node, transformation_weight, pdf_infl, transformation_infl

        # holds the last information of size length
        self.context = []

        # holds the last concepts
        self.last_concepts = []

        # holds the hidden relation between states and concepts
        self.transformations = [[] for _ in range(self.CONCEPT_SIZE)]
        self.transformation_freq = [[] for _ in range(self.CONCEPT_SIZE)]
        self.transformation_totals = [[] for _ in range(self.CONCEPT_SIZE)]

        # for dynamic programming
        self.memoized = {}
        self.MEMOIZED_SIZE = 256

    def addToContext(self, data):
        if len(self.context) == self.SIZE:
            self.context.pop()
        
        self.context.insert(0, data)
        return

    def getMaxProbabilityStates(self, data):
        ret = self.getMemoized('getMaxProbabilityStates', (data))
        if ret != None:
            return ret

        mps = {}
        for li, concepts in enumerate(self.node_freq):
            for ci, states in enumerate(concepts):
                max_probability = max(states)
                max_probability_states = [ix for ix, x in enumerate(states) if x == max_probability]
                if data in max_probability_states:
                    mps[(li, ci)] = max_probability_states

        self.updateMemoized('getMaxProbabilityStates', (data), mps)
        return mps

    def getConcepts(self, reverse=False):
        val = '0' if reverse else '1'
        concepts = []

        # get concept range based on precent concept as the context might not be up to max
        concept_range = sum([2**x for x in range(len(self.context))])
        
        for n in range(concept_range):
            concept = tuple([self.context[i] for i, x in enumerate(list(toBin(n+1))[::-1]) if x == val])
            if concept == tuple([]):
                continue

            concepts.append(concept)
        return concepts

    def getMemoized(self, function_name, arguments):
        if function_name not in self.memoized:
            self.memoized[function_name] = {}
            
        if arguments not in self.memoized[function_name]:
            return
        return self.memoized[function_name][arguments]
    
    def getPredictedOutputs(self, processes, return_max_weight=False):
        m = max(processes)
        predicted_outputs = [state for state, x in enumerate(processes) if x >= m]
        if return_max_weight:
            return predicted_outputs, m

        else:
            return predicted_outputs

    def getTransformations(self, concept_x, concept_y):
        concepts = [concept_y]
        for ix, cx in enumerate(concept_x):
            new_concepts = []
            for concept in concepts:
                for iy, cy in enumerate(concept):
                    if type(cy) == str:
                        continue
                    
                    if cx == cy:
                        c_new = list(concept)
                        c_new[iy] = str(ix)
                        new_concepts.append(tuple(c_new))
            concepts = new_concepts.copy()
        return new_concepts
        
    def log(self, output, title=None):
        if not self.log_state:
            return

        log(output, title)
        return

    def mean(self, li):
        if len(li) > 0:
            return sum(li)/len(li)

        else:
            return 0

    def normalize(self, li):
        s = sum(li) if type(li) != dict else sum(li.values())
        if s > 0:
            if type(li) == dict:
                return {x:li[x]/s for x in li}

            else:
                return [x/s for x in li]

        else:
            if type(li) == dict:
                return {x:0 for x in li}

            else:
                return [0 for _ in li]

    # @jit(nopython=True) # Set "nopython" mode for best performance, equivalent to @njit
    def process(self, data):
        # update the node network
        self.update(data)

        # add data to context
        self.addToContext(data)
        
        # used to model
        self.models_used = [[] for _ in range(self.STATE_SIZE)]

# ----------------------the process instance-------------------------------------
        # the processes construct
        # the prob densities and the inluences
        pdf_processes = [[] for _ in range(self.STATE_SIZE)]
        
        # the transf infl and densities
        transformation_processes = [[] for _ in range(self.STATE_SIZE)]

        # get the states at this instance
        concepts = self.getConcepts()

        for i, concept in enumerate(concepts):
            if concept not in self.nodes[i]:
                self.nodes[i].append(concept)
                self.node_freq[i].append([0.0 for _ in range(self.STATE_SIZE)])
                
                self.transformations[i].append([])
                self.transformation_totals[i].append([])
                self.transformation_freq[i].append([])

                self.totals[i].append(0.0)

            # the id of the concept
            concept_index = self.nodes[i].index(concept)

            # pdf weight for relative value
            total = self.totals[i][concept_index]
            
            # increment the node
            self.totals[i][concept_index] += 1


# ---------------------------------------------------------------probability density function--------------
            for state, freq in enumerate(self.node_freq[i][concept_index]):
                weight = freq/total if total > 0 else 0
                pdf_processes[state].append(weight)

# --------------------------------------------------transformations------------------------------------------------------------------------
            for ti, transformation_ in enumerate(self.transformations[i][concept_index]):
                transformation_total = self.transformation_totals[i][concept_index][ti]
                transformation_weights = []
                
                for context_index, context_value in enumerate(self.context):
                    transformation_freq = self.transformation_freq[i][concept_index][ti][context_index][context_value]
                    transformation_weight = transformation_freq / transformation_total if transformation_total > 0 else 0.0
                    transformation_weights.append(transformation_weight)

                transformation_weight = self.mean(transformation_weights)
                transformation, level = transformation_

                concept_transform = self.solveTransformation(transformation, concepts[i])
                
                if concept_transform not in self.nodes[level]:
                    continue

                concept_transform_index = self.nodes[level].index(concept_transform)

                # probability distribution of concept to states
                concept2states_freq = self.node_freq[level][concept_transform_index]
                max_concept2states_freq = max(concept2states_freq)

                for state, freq in enumerate(concept2states_freq):
                    if max_concept2states_freq == freq:
                        # if len(self.context) > 2 and self.context[-1] == 49 and self.context[-2] == 57 and self.context[-3] == 96 and state == 50:
                        #     print('context = {}-{}, transformation = {}-{}-({}/{}={})'.format(
                        #         context_index, context_value, transformation_, concept_transform, transformation_freq, transformation_total,
                        #         transformation_weight))
                        transformation_processes[state].append(transformation_weight)
                
        # max_length = max([len(x) for x in pdf_processes])
        # processes = [self.mean([xx for xx in x if xx >= self.mean(x)]) * self.trustFactor(max_length) for x in pdf_processes]
        # processes = [self.mean(x) for x in transformation_processes]
        processes = [self.mean([xx for xx in x if xx >= self.mean(x)]) for x in transformation_processes]
        # print(processes)
        
        predicted_outputs, max_weight = self.getPredictedOutputs(processes, True)
        
        self.last_concepts = concepts.copy()
        return predicted_outputs, max_weight, processes

    def resetMemoized(self, function_name):
        if function_name not in self.memoized:
            self.memoized[function_name] = {}
            return

        self.memoized.pop(function_name)
        self.memoized[function_name] = {}
        return 
            
    def solveTransformation(self, transformation, concept):
        transform = list(transformation)
        for i, t in enumerate(transformation):
            if type(t) != str:
                continue

            index = int(t)
            transform[i] = concept[index]
        return tuple(transform)

    def trustFactor(self, x):
        if type(x) == list:
            x = len(x)
        return (1 + x)**-1 

    # @jit(nopython=True) # Set "nopython" mode for best performance, equivalent to @njit
    def update(self, data):
        max_probability_states = self.getMaxProbabilityStates(data)       
        for i, concept in enumerate(self.last_concepts):
            index = self.nodes[i].index(concept)
            self.node_freq[i][index][data] += 1

            # create transformation weights
            for x in max_probability_states:
                level, concept_index = x
                other_concept = self.nodes[level][concept_index]
                factor = len(max_probability_states[x])**-1

                if not all([x in other_concept for x in concept]):
                    continue

                transformations = self.getTransformations(concept, other_concept)
                for transformation in transformations:
                    transformation_ = (transformation, level)

                    if transformation_ not in self.transformations:
                        self.transformations[i][index].append(transformation_)
                        self.transformation_freq[i][index].append([[0.0 for ssi in range(self.STATE_SIZE)] for si in range(self.SIZE)])
                        self.transformation_totals[i][index].append(0.0)

                    ti = self.transformations[i][index].index(transformation_)
                
                    # updates the transformation weights
                    for context_index, context_value in enumerate(self.context):
                        self.transformation_freq[i][index][ti][context_index][context_value] += 1
                    self.transformation_totals[i][index][ti] += 1
        return

    def updateMemoized(self, function_name, arguments, value):
        if function_name not in self.memoized:
            self.memoized[function_name] = {}

        vals = self.memoized[function_name]
        length = len(vals)
        if length == self.MEMOIZED_SIZE:
            self.memoized[function_name].pop([x for x in vals.keys()][random.randint(length)])

        self.memoized[function_name][arguments] = value
        return

'''
Author: Joshua, Christian r0b0tx
Date: 1 Nov 2018
File: main.py
Project: GHOST
'''

# initialize objects
PROCESSOR = Processor()

def main():
    # td = [learn_counting(101)]
    td = [learn_counting(11, 3), learn_counting(21, 3), learn_counting(31, 2), learn_counting(51, 2), learn_counting(61), learn_counting(101)]
    # td = [learn_counting(21), learn_counting(11), train('train.old.txt'), learn_counting(11), train('train.old.txt')]

    # initialize
    last_outputs = []
    last_input_data = None
    weight = 0
    po = None
    
    for training_data in td:
        for c in training_data:
            if weight > 0: print('x = {}, y = {}, y_pred = {}, weight = {}, '.format(last_input_data, c, last_outputs, weight))

            data = ord(c)

            outputs, weight, po = PROCESSOR.process(data)
            outputs = [str(chr(x)).encode('utf-8') for x in outputs]
            
            last_outputs = outputs.copy()
            last_input_data = c
if __name__ == '__main__':
    main()

