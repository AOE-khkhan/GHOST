{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Using Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will understand the siamese network by building the face recognition model. The objective of our network is to understand whether two faces are similar or dissimilar. We use AT & T's the Database of Faces which can be downloaded from here (https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html)\n",
    "\n",
    "Once you have downloaded and extracted the archive, you can see the folders like s1, s2 up to s40 as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define another function get_data for generating our data. As we know, for the Siamese network, data should be in the form of pairs (genuine and imposite) with a binary label.\n",
    "\n",
    "First, we read the images (img1, img2) from the same directory and store them in the x_genuine_pair array and assign y_genuine to 1. Next, we read the images (img1, img2) from the different directory and store them in the x_imposite pair and assign y_imposite to 0.\n",
    "\n",
    "Finally, we concatenate both x_genuine_pair, x_imposite to X and y_genuine, y_imposite to Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(ch=9):\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    #character images\n",
    "    ch_images = x_train[y_train == ch]\n",
    "    \n",
    "    #total size\n",
    "    total_sample_size = len(ch_images)\n",
    "    \n",
    "    #get the new size\n",
    "    dim1 = x_train[0].shape[0]\n",
    "    dim2 = x_train[0].shape[1]\n",
    "\n",
    "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
    "    x_geuine_pair = np.zeros([total_sample_size, 2, dim1, dim2, 1])  # 2 is for pairs\n",
    "    y_genuine = np.ones([total_sample_size, 1])\n",
    "    \n",
    "    indices = list(range(total_sample_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    #store the images to the initialized numpy array\n",
    "    x_geuine_pair[:, 0, :, :, 0] = ch_images\n",
    "    x_geuine_pair[:, 1, :, :, 0] = ch_images[indices]\n",
    "\n",
    "    #the dis-similar ones\n",
    "    x_imposite_pair = np.zeros([total_sample_size, 2, dim1, dim2, 1])\n",
    "    y_imposite = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    np.random.shuffle(indices) #shuffle again\n",
    "    x_imposite_pair[:, 0, :, :, 0] = ch_images[indices]\n",
    "    x_imposite_pair[:, 1, :, :, 0] = x_train[y_train != ch][:total_sample_size]\n",
    "            \n",
    "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
    "    X = ((np.concatenate([x_geuine_pair, x_imposite_pair], axis=0) / 255 ) > 0.5).astype(np.int8)\n",
    "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
    "    \n",
    "    indices = list(range(2*total_sample_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return X[indices], Y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11898, 2, 28, 28, 1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11898, 1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a03fbc26a0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[4, 1, :, :, 0], vmin=0, vmax=1, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split our data for training and testing with 75% training and 25% testing proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that, we have successfully generated our data, we build our siamese network. First, we define the base network which is basically a convolutional network used for feature extraction. We build two convolutional layers with rectified linear unit (ReLU) activations and max pooling followed by flat layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_network(input_shape):\n",
    "    \n",
    "    seq = Sequential()\n",
    "    \n",
    "    nb_filter = [6, 12]\n",
    "    kernel_size = 3\n",
    "    \n",
    "    \n",
    "    #convolutional layer 1\n",
    "    seq.add(Conv2D(nb_filter[0], (kernel_size, kernel_size), input_shape=input_shape, padding='valid'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    seq.add(Dropout(.25))\n",
    "    \n",
    "    #convolutional layer 2\n",
    "    seq.add(Conv2D(nb_filter[1], (kernel_size, kernel_size), padding='valid'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "    seq.add(Dropout(.25))\n",
    "\n",
    "    #flatten \n",
    "    seq.add(Flatten())\n",
    "    seq.add(Dense(128, activation='relu'))\n",
    "    seq.add(Dropout(0.1))\n",
    "    seq.add(Dense(50, activation='relu'))\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we feed the image pair, to the base network, which will return the embeddings that is, feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[2:]\n",
    "img_a = Input(shape=input_dim)\n",
    "img_b = Input(shape=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network = build_base_network(input_dim)\n",
    "feat_vecs_a = base_network(img_a)\n",
    "feat_vecs_b = base_network(img_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feat_vecs_a and feat_vecs_b are the feature vectors of our image pair. Next, we feed this feature vectors to the energy function to compute the distance between them, we use Euclidean distance as our energy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we set the epoch length to 13 and we use RMS prop for optimization and define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 64\n",
    "rms = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[img_a, img_b], outputs=distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function as contrastive_loss function and compile the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    return labels[predictions.ravel() < 0.5].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=contrastive_loss, optimizer=rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate our data and check our data size. As you can see we have 20,000 data points, out of these 10,000 are genuine pairs and 10,000 are imposite pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = x_train[start:start+step, 0]\n",
    "img2 = x_train[start:start+step, 1]\n",
    "similarity = y_train[start:start+step, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75 samples, validate on 25 samples\n",
      "Epoch 1/64\n",
      " - 1s - loss: 0.0304 - val_loss: 0.0894\n",
      "Epoch 2/64\n",
      " - 0s - loss: 0.0434 - val_loss: 0.0864\n",
      "Epoch 3/64\n",
      " - 0s - loss: 0.0279 - val_loss: 0.0733\n",
      "Epoch 4/64\n",
      " - 0s - loss: 0.0143 - val_loss: 0.0808\n",
      "Epoch 5/64\n",
      " - 0s - loss: 0.0224 - val_loss: 0.0862\n",
      "Epoch 6/64\n",
      " - 0s - loss: 0.0129 - val_loss: 0.0806\n",
      "Epoch 7/64\n",
      " - 0s - loss: 0.0298 - val_loss: 0.0926\n",
      "Epoch 8/64\n",
      " - 0s - loss: 0.0283 - val_loss: 0.0884\n",
      "Epoch 9/64\n",
      " - 0s - loss: 0.0075 - val_loss: 0.0999\n",
      "Epoch 10/64\n",
      " - 0s - loss: 0.0235 - val_loss: 0.0643\n",
      "Epoch 11/64\n",
      " - 0s - loss: 0.0171 - val_loss: 0.1113\n",
      "Epoch 12/64\n",
      " - 0s - loss: 0.0246 - val_loss: 0.0846\n",
      "Epoch 13/64\n",
      " - 0s - loss: 0.0149 - val_loss: 0.1014\n",
      "Epoch 14/64\n",
      " - 0s - loss: 0.0284 - val_loss: 0.0972\n",
      "Epoch 15/64\n",
      " - 0s - loss: 0.0128 - val_loss: 0.0817\n",
      "Epoch 16/64\n",
      " - 0s - loss: 0.0131 - val_loss: 0.0823\n",
      "Epoch 17/64\n",
      " - 0s - loss: 0.0069 - val_loss: 0.0857\n",
      "Epoch 18/64\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0834\n",
      "Epoch 19/64\n",
      " - 0s - loss: 0.0202 - val_loss: 0.0792\n",
      "Epoch 20/64\n",
      " - 0s - loss: 0.0196 - val_loss: 0.0732\n",
      "Epoch 21/64\n",
      " - 0s - loss: 0.0111 - val_loss: 0.0858\n",
      "Epoch 22/64\n",
      " - 0s - loss: 0.0022 - val_loss: 0.0900\n",
      "Epoch 23/64\n",
      " - 0s - loss: 0.0136 - val_loss: 0.0826\n",
      "Epoch 24/64\n",
      " - 0s - loss: 0.0093 - val_loss: 0.0999\n",
      "Epoch 25/64\n",
      " - 0s - loss: 0.0265 - val_loss: 0.0828\n",
      "Epoch 26/64\n",
      " - 0s - loss: 0.0058 - val_loss: 0.0964\n",
      "Epoch 27/64\n",
      " - 0s - loss: 0.0104 - val_loss: 0.0825\n",
      "Epoch 28/64\n",
      " - 0s - loss: 0.0091 - val_loss: 0.0906\n",
      "Epoch 29/64\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0856\n",
      "Epoch 30/64\n",
      " - 0s - loss: 0.0120 - val_loss: 0.0827\n",
      "Epoch 31/64\n",
      " - 0s - loss: 0.0129 - val_loss: 0.0822\n",
      "Epoch 32/64\n",
      " - 0s - loss: 0.0046 - val_loss: 0.0869\n",
      "Epoch 33/64\n",
      " - 0s - loss: 0.0024 - val_loss: 0.0912\n",
      "Epoch 34/64\n",
      " - 0s - loss: 0.0072 - val_loss: 0.0930\n",
      "Epoch 35/64\n",
      " - 0s - loss: 0.0036 - val_loss: 0.0803\n",
      "Epoch 36/64\n",
      " - 0s - loss: 0.0109 - val_loss: 0.0794\n",
      "Epoch 37/64\n",
      " - 0s - loss: 0.0049 - val_loss: 0.0706\n",
      "Epoch 38/64\n",
      " - 0s - loss: 0.0104 - val_loss: 0.0738\n",
      "Epoch 39/64\n",
      " - 0s - loss: 0.0231 - val_loss: 0.0857\n",
      "Epoch 40/64\n",
      " - 0s - loss: 0.0055 - val_loss: 0.0713\n",
      "Epoch 41/64\n",
      " - 0s - loss: 0.0058 - val_loss: 0.0872\n",
      "Epoch 42/64\n",
      " - 0s - loss: 0.0026 - val_loss: 0.0820\n",
      "Epoch 43/64\n",
      " - 0s - loss: 0.0084 - val_loss: 0.0924\n",
      "Epoch 44/64\n",
      " - 0s - loss: 0.0115 - val_loss: 0.0814\n",
      "Epoch 45/64\n",
      " - 0s - loss: 0.0146 - val_loss: 0.0796\n",
      "Epoch 46/64\n",
      " - 0s - loss: 0.0094 - val_loss: 0.0753\n",
      "Epoch 47/64\n",
      " - 0s - loss: 0.0043 - val_loss: 0.0782\n",
      "Epoch 48/64\n",
      " - 0s - loss: 0.0038 - val_loss: 0.0789\n",
      "Epoch 49/64\n",
      " - 0s - loss: 0.0054 - val_loss: 0.0754\n",
      "Epoch 50/64\n",
      " - 0s - loss: 0.0024 - val_loss: 0.1081\n",
      "Epoch 51/64\n",
      " - 0s - loss: 0.0083 - val_loss: 0.0653\n",
      "Epoch 52/64\n",
      " - 0s - loss: 0.0053 - val_loss: 0.0747\n",
      "Epoch 53/64\n",
      " - 0s - loss: 0.0086 - val_loss: 0.0779\n",
      "Epoch 54/64\n",
      " - 0s - loss: 0.0038 - val_loss: 0.0708\n",
      "Epoch 55/64\n",
      " - 0s - loss: 0.0054 - val_loss: 0.0908\n",
      "Epoch 56/64\n",
      " - 0s - loss: 0.0261 - val_loss: 0.0723\n",
      "Epoch 57/64\n",
      " - 0s - loss: 0.0021 - val_loss: 0.0696\n",
      "Epoch 58/64\n",
      " - 0s - loss: 0.0025 - val_loss: 0.0798\n",
      "Epoch 59/64\n",
      " - 0s - loss: 0.0049 - val_loss: 0.0693\n",
      "Epoch 60/64\n",
      " - 0s - loss: 6.3517e-04 - val_loss: 0.0747\n",
      "Epoch 61/64\n",
      " - 0s - loss: 0.0078 - val_loss: 0.0766\n",
      "Epoch 62/64\n",
      " - 0s - loss: 1.8374e-04 - val_loss: 0.0793\n",
      "Epoch 63/64\n",
      " - 0s - loss: 0.0080 - val_loss: 0.0531\n",
      "Epoch 64/64\n",
      " - 0s - loss: 0.0154 - val_loss: 0.0853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2a03ee176d8>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([img_1, img2], similarity, validation_split=.25, batch_size=16, verbose=2, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we make predictions with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7641968631692807"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict([x_test[:, 0], x_test[:, 1]])\n",
    "compute_accuracy(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 50)           45698       input_17[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           sequential_8[1][0]               \n",
      "                                                                 sequential_8[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 45,698\n",
      "Trainable params: 45,698\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
