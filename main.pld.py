# -*- coding: utf-8 -*-
"""GHOST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iMcdaYlNYcGoQJLCharUtCE7Ev2AbfYI
"""
import random

def toBin(c, n=8):
    if type(c) == str:
        c = ord(c)
        
    b = bin(c)[2:]
    return ''.join(['0' for _ in range(n - len(b))]) + b

def toChar(b):
    n = int('0b{}'.format(b), 2)
    return n.to_bytes((n.bit_length() + 7) // 8, 'big').decode()

def train(filepath):
    with open(filepath, 'r') as f:
        lines = f.readlines()
        for line in lines:
            for s in list(line.strip()):
                yield s, [s]
            yield ['`'], ['`']

def learn_movement(n_iter=10, actions=None, size=3, start=[0,0]):
    # define the position in world
    position = start
    last_position = position.copy()

    # get the size of the world
    size = [size, size] if type(size) == int else size
    world = [['_' for __ in range(size[0])] for _ in range(size[0])]

    # place the agent in the begining of the world
    world[start[0]][start[1]] = 'A'

    # get the actions in world
    actions = 'RLUD' if actions == None else actions
    actions = list(actions)

    # define conditions in world
    conditions = {'R':[0, 1], 'L':[0, -1], 'U':[1, 1], 'D':[1, -1]}
    for _ in range(n_iter):
        for __ in actions:
            action = actions[random.randint(0, len(actions)-1)]
            index, inc = conditions[action]
            pos_temp = position[index] + inc

            if pos_temp in list(range(0, size[index], 1)):
                # update the position in world
                world[position[0]][position[1]] = '_'

                position[index] = pos_temp

                world[position[0]][position[1]] = 'A'

            for action in [action, 'N']:
                ret = [action] + [chr(x) for x in last_position]
                last_position = position.copy()
                yield ret, ret#[x for xx in world for x in xx]


def learn_counting(n=101, n_iter=1):
    for i in range(5):
        yield ['`'], ['`']

    for _ in range(n_iter):
        print('\nthis is iteration {} of {} iteration(s): counting to {}\n'.format(_+1, n_iter, n))
        for i in range(n):
            data = str(i)
            for c in list(data):
                yield [c], [c]
            yield ['`'], ['`']

def log(output='', title=None):
    if type(output) in [str, int, type(None)]:
        print('{} = {}'.format(title, output))
        return

    if title != None:
        print('\n{} \n{}'.format(title, ''.join(['=' for _ in range(len(title)+2)])))

    print('{}\n'.format(output))
    return


'''
Author: Joshua, Christian r0b0tx
Date: 1 Nov 2018
File: processor.py
Project: GHOST
'''

# import from python standard lib
from itertools import combinations

class Processor:
    """docstring for Processor"""    
    
    def __init__(self, state_size=8, size=3, no_transformation=False):
        self.no_transformation = no_transformation

        # if to show output
        self.log_state = True

        # processor size in bits
        self.SIZE = size
        self.CONCEPT_SIZE = sum([2**x for x in range(self.SIZE)])

        # sensory binary data size
        self.STATE_SIZE = 2**state_size

        # matrix of nodes that process and project data
        self.nodes = []
        self.node_state_freq = []
        self.node_npdf_freq = [] #the freq of the concept being undecidable by pdf

        # holds the last information of size length
        self.context = []

        # holds the last concepts
        self.last_concepts = []
        self.last_concept_indices = []

        # holds the hidden relation between states and concepts
        self.node_transformations = []
        self.node_transformation_freq = []

        # used to model
        self.last_pdf_predictions = []
        self.last_transformation_models = []

        # the concept nodes index that are not found in the nodes list
        self.last_concepts_node_indicies_not_found = []
        
        # for dynamic programming
        self.memoized = {}
        self.MEMOIZED_SIZE = 256


    def addToContext(self, data):
        if len(self.context) == self.SIZE:
            self.context.pop()
        
        self.context.insert(0, data)

        return

    def getMaxProbabilityStates(self, output_index, data):
        mps = {}
        for concept_index, concept in enumerate(self.nodes):
            states = self.node_state_freq[concept_index][output_index]
            max_probability = max(states)
            max_probability_states = [ix for ix, x in enumerate(states) if x == max_probability]
            if data in max_probability_states:
                mps[concept] = max_probability_states

        return mps

    def getConcepts(self, context=None, size=None, reverse=False):
        normal_concepts = self.generateConcepts(context, size, reverse)

        # avoid repeating elements of concept consideration
        return normal_concepts

        context = self.context.copy()
        context_set = list(set(context))

        if len(context_set) == len(context):
            return normal_concepts

        for i, c in enumerate(context_set):
            if context.count(c) < 2:
                continue

            while c in context:
                context[context.index(c)] = str(i)

        other_concepts = self.generateConcepts(context, size, reverse)
        
        concepts = [] + normal_concepts
        for concept in other_concepts:
            if concept not in concepts:
                concepts.append(concept)

        return concepts

    def generateConcepts(self, context=None, size=None, reverse=False):
        if context == None: context = self.context.copy()
        if size == None: size = self.SIZE

        if reverse and len(context) < size:
            return []

        val = '0' if reverse else '1'
        concepts = []

        # get concept range based on precent concept as the context might not be up to max
        concept_range = sum([2**x for x in range(len(context))])
        m = size - 1
        for n in range(concept_range):
            bin_li = list(toBin(n+1, size))
            concept = [context[m - i] for i, x in enumerate(bin_li) if x == val]
            if reverse:
                concept.reverse()
            concept = tuple(concept)
            concepts.append(concept)
        return concepts

    def getMaxValueIndices(self, processes, return_max_weight=False):
        m = max(processes) if len(processes) > 0 else 0
        predicted_outputs = [state for state, x in enumerate(processes) if x == m]
        if return_max_weight:
            return predicted_outputs, m

        else:
            return predicted_outputs

    def getTransformations(self, concept_x, concept_y):
        concepts = [concept_y]
        for ix, cx in enumerate(concept_x):
            new_concepts = []
            for concept in concepts:
                for iy, cy in enumerate(concept):
                    if type(cy) == str:
                        continue
                    
                    if cx == cy:
                        c_new = list(concept)
                        c_new[iy] = str(ix)
                        new_concepts.append(tuple(c_new))

            if len(new_concepts) > 0:
                concepts = new_concepts.copy()
        return concepts
        
    def log(self, output, title=None):
        if not self.log_state:
            return

        log(output, title)
        return

    def mean(self, li):
        if len(li) > 0:
            return sum(li)/len(li)

        else:
            return 0

    def normalize(self, li):
        s = sum(li) if type(li) != dict else sum(li.values())
        if s > 0:
            if type(li) == dict:
                return {x:li[x]/s for x in li}

            else:
                return [x/s for x in li]

        else:
            if type(li) == dict:
                return {x:0 for x in li}

            else:
                return [0 for _ in li]

    def process(self, state, input_data):
        # the variables
        pdf_processes, transformation_processes = [], []
        transformation_models, pdf_solvable = [], []
        transformation_predicted_outputs, pdf_predicted_outputs, predicted_outputs = [], [], []
        pdf_max_weight, transformation_max_weight, max_weight = [], [], []
        po = []
        
        if len(self.context) < self.SIZE:
            self.addToContext(state)
            return predicted_outputs, [format(mw, '.4f') for mw in max_weight], po

        # update the node network
        for output_index, data in enumerate(input_data):
            self.update(output_index, data)

            pdf_processes.append([])
            transformation_processes.append([])
            transformation_models.append([])

            # check if pdf can compute solution
            pdf_solvable.append(True)

            # variables to perform result computation
            transformation_predicted_outputs.append([]), pdf_predicted_outputs.append([]), predicted_outputs.append([])
            pdf_max_weight.append(0), transformation_max_weight.append(0), max_weight.append(0)

# -------------------------------------------the process instance-------------------------------------
            for _ in range(self.STATE_SIZE):
                # track the processesing
                pdf_processes[output_index].append(0)
                transformation_processes[output_index].append(0)

                transformation_models[output_index].append([])

        # add data to context
        self.addToContext(state)

        # get the states at this instance
        concepts = self.getConcepts()

        # index of the concepts
        concept_node_indices = []
        concepts_node_indicies_not_found = []

        for concept_index, concept in enumerate(concepts):
            concept_model = (concept_index, concept)
            compute_pdf = True
            if concept_model not in self.nodes:
                # add concepts
                self.nodes.append(concept_model)

                # the id of the concept
                concept_node_index = self.nodes.index(concept_model)

                # save indices
                concept_node_indices.append(concept_node_index)
                concepts_node_indicies_not_found.append(concept_node_index)

                compute_pdf = False

                 # matrix of concpts to states
                self.node_state_freq.append([]), self.node_npdf_freq.append([])
                
                # matrix of concepts to transformations        
                self.node_transformations.append([]), self.node_transformation_freq.append([])

                for output_index, data in enumerate(input_data):
                    # matrix of concpts to states
                    self.node_state_freq[concept_node_index].append([0.0 for _ in range(self.STATE_SIZE)])
                    self.node_npdf_freq[concept_node_index].append([0.0 for _ in range(2)])
                    
                    # matrix of concepts to transformations        
                    self.node_transformations[concept_node_index].append([])
                    self.node_transformation_freq[concept_node_index].append([])

            for output_index, data in enumerate(input_data):
# ===============================================probability prediction===============================
                if compute_pdf:

                    # the id of the concept
                    concept_node_index = self.nodes.index(concept_model)
                    
                    # save indices
                    concept_node_indices.append(concept_node_index)

                    # check the no pdf weight to see if the concept can not be solved by the pdf
                    a, b = self.node_npdf_freq[concept_node_index][output_index]
                    c = a / b if b > 0 else 0

                    # if self.context[-2] == 57 and self.context[-3] == 96:
                    #     print(concept_model, c)

                    # check if it is pdf solvable
                    pdf_solvable[output_index] = True if c <= 0.5 and pdf_solvable[output_index] == True else False
                
                    # the max states:Purpose is to avoid noise
                    concept2states_freq, state_weight = self.getMaxValueIndices(self.normalize(self.node_state_freq[concept_node_index][output_index]), True)
                    
                    factor = len(concept2states_freq)**-1

                    for state in concept2states_freq:
                        weight = state_weight * factor

                        # include the weight in the processes
                        if weight > pdf_processes[output_index][state]:
                            pdf_processes[output_index][state] = weight

                            # if len(self.context) > 2 and self.context[-2] == 57 and self.context[-3] == 96:
                            #     print('cm = {}, state = {}, concept_weight = {} * {} = {}'.format(concept_model, state, state_weight, factor, weight))
                if self.no_transformation:
                    continue
    # ==============================================transformations================================================
                transformations = self.node_transformations[concept_node_index][output_index]

                tf = self.node_transformation_freq[concept_node_index][output_index]      #transformation freq

                # teh influence of the concept to transformation: decides the best transf for a concept
                transformation_weights = self.normalize(tf)

                ttf = sum(tf)
                transformation_weights = [tw * self.trustFactor(ttf) for tw in transformation_weights]

                max_transformation_weight_ids, max_transformation_weight = self.getMaxValueIndices(transformation_weights, True)

                for transformation_index in range(len(transformations)):
                # for transformation_index in max_transformation_weight_ids:
                    # transformation influence
                    transformation_weight = transformation_weights[transformation_index]

                    # the transformation model
                    transformation_model = transformations[transformation_index]

                    # the tarsnformation model index
                    transformation_model_id = (concept_node_index, transformation_index)

                    # the ids for the transformations
                    transformation, concept_y_index = transformation_model

                    # transform the transformation
                    concept_transform = self.solveTransformation(transformation, concepts[-1])

                    concept_transform_model = (concept_y_index, concept_transform)
                    if concept_transform_model not in self.nodes:
                        continue

                    # concept transf index
                    concept_transform_node_index = self.nodes.index(concept_transform_model)

                    # the max states:Purpose is to avoid noise
                    concept2states_freq, state_weight = self.getMaxValueIndices(self.normalize(self.node_state_freq[concept_transform_node_index][output_index]), True)
                    
                    factor = len(concept2states_freq)**-1
                    
                    # avoid noise of un influencial dataset
                    if state_weight == 0:
                        continue

                    for state in concept2states_freq:
                        # weight = max_transformation_weight * state_weight
                        weight = transformation_weights[transformation_index] * state_weight
                        transformation_models[output_index][state].append((concept_node_index, transformation_index))

                        if weight > transformation_processes[output_index][state]:
                            # track the transforation process
                            transformation_processes[output_index][state] = weight

                            # if self.context[-3] == 96 and self.context[-2] == 48:
                            # if self.context[-2] == 57 and self.context[-3] == 96:
                            # if self.context[-1] == 57 and self.context[-2] == 48 and self.context[-3] == 96:
                            #     print('concept = {}-{}, transf_weight => {} / {} = {} * {} = {}-{}, ct = {}, s= {}-{}'.format(
                            #         concept_index, concepts[concept_index], format(tf[transformation_index], '.3f'), format(sum(tf), '.3f'), format(transformation_weight, '.3f'), format(state_weight, '.3f'), format(weight, '.3f'), format(factor, '.3f'), concept_transform_model, transformation_model, state
                            #         )
                            #     )

        for output_index, data in enumerate(input_data):
            # if all the concepts are found then teh pdf solvable does not matter
            all_concepts_found = True if len(concepts_node_indicies_not_found) == 0 else False
            pdf_solvable[output_index] = True if all_concepts_found else pdf_solvable[output_index]

            # get the max vals
            pdf_predicted_outputs[output_index], pdf_max_weight[output_index] = self.getMaxValueIndices(pdf_processes[output_index], True)
            transformation_predicted_outputs[output_index], transformation_max_weight[output_index] = self.getMaxValueIndices(transformation_processes[output_index], True)
            
            # decide the algorithm value to use
            if pdf_solvable[output_index]:
                predicted_outputs[output_index], max_weight[output_index] = (pdf_predicted_outputs[output_index].copy(), pdf_max_weight[output_index])
            
            else:
                predicted_outputs[output_index], max_weight[output_index] = (transformation_predicted_outputs[output_index].copy(), transformation_max_weight[output_index])
            
        po = pdf_solvable.copy()

        self.last_concepts = concepts.copy()
        self.last_concept_indices = concept_node_indices.copy()
        self.last_pdf_predictions = pdf_predicted_outputs.copy()
        self.last_concepts_node_indicies_not_found = concepts_node_indicies_not_found.copy()
        self.last_transformation_models = transformation_models.copy()
        return predicted_outputs, [format(mw, '.4f') for mw in max_weight], po

    def solveTransformation(self, transformation, concept):
        transform = list(transformation)
        for i, t in enumerate(transformation):
            if type(t) != str:
                continue

            index = int(t)
            transform[i] = concept[index]
        return tuple(transform)

    def trustFactor(self, x):
        if type(x) == list:
            x = len(x)
        return x / (1 + x) 

    def update(self, output_index, data):
        # get the nodes that match the reply in pdf
        max_probability_states = self.getMaxProbabilityStates(output_index, data)    

        # get the prvious values
        last_pdf_predictions = self.last_pdf_predictions[output_index] if output_index in self.last_pdf_predictions else []
        last_concepts_node_indicies_not_found = lcinf = self.last_concepts_node_indicies_not_found

        last_concepts_node_indicies_found = [cni for cni in self.last_concept_indices if cni not in lcinf]
        all_concepts_found = True if len(last_concepts_node_indicies_not_found) == 0 else False

        last_transformation_models = self.last_transformation_models[output_index][data] if len(self.last_transformation_models) > 0 else []
# ======================================updating pdf, pdf_concept and creating transformations===============        
        for concept_index, concept in enumerate(self.last_concepts):
            concept_model = (concept_index, concept)
            concept_node_index = self.nodes.index(concept_model)

# =====================================increment the node for pdf===============================
            self.node_state_freq[concept_node_index][output_index][data] += 1

# =========================================increment the node for not pdf solvable==============================
            if not all_concepts_found:
                self.node_npdf_freq[concept_node_index][output_index][0] += 1 if data not in last_pdf_predictions else 0
                self.node_npdf_freq[concept_node_index][output_index][1] += 1
            
            if self.no_transformation:
                continue

# ========================create transformation and update transf weights=========================================
            # at transformation is only fromed when there are missing concepts
            if all_concepts_found:
                continue

            for cni in last_concepts_node_indicies_found:
                for concept_model in max_probability_states:
                    level, other_concept = concept_model
                    factor = len(max_probability_states[concept_model])**-1

                    if factor < 1:
                        continue

                    if not all([x in other_concept for x in concept]):
                        continue

                    transformations = self.getTransformations(self.last_concepts[-1], other_concept)
                    for transformation in transformations:
                        transformation_model = (transformation, level)

                        if transformation_model not in self.node_transformations[cni][output_index]:
                            # if self.context[-3] == 96 and self.context[-2] == 48:
                            #     print(transformation_model, concept, other_concept)
                            self.node_transformations[cni][output_index].append(transformation_model)
                            self.node_transformation_freq[cni][output_index].append(0)
                        
                        # increment the value
                        transformation_index = self.node_transformations[cni][output_index].index(transformation_model)
                        if (cni, transformation_index) in last_transformation_models:
                            self.node_transformation_freq[cni][output_index][transformation_index] += 1

        return

'''
Author: Joshua, Christian r0b0tx
Date: 1 Nov 2018
File: main.py
Project: GHOST
'''

# initialize objects for motion
# PROCESSOR = [Processor(state_size=8, size=3, no_transformation=True) for _ in range(1)]

# initialize for counting
PROCESSOR = [Processor(state_size=8, size=3, no_transformation=True) for _ in range(3)]

def main():
    td = [learn_movement(25, size=9)]
    # td = [learn_counting(15, 3), learn_counting(25, 2), learn_counting(35, 1), learn_counting(55, 1),learn_counting(75, 1), learn_counting(105, 1)]
    # td = [learn_counting(21), learn_counting(11), train('train.old.txt'), learn_counting(11), train('train.old.txt')]

    # initialize
    last_outputs = []
    last_input_data = None
    weight = 0
    po = None
    
    for training_data in td:
        for sensory_data, input_data in training_data:
            for i, state in enumerate(sensory_data):
                s = state if i == 0 else ord(state)
                inp_data = [x if ix == 0 else ord(x) for ix, x in enumerate(input_data)]

                last_outputs = ['a lot' if weight[i] == 0 or len(x) > 9 else x for i, x in enumerate(last_outputs)]
                print('sensor_value = {}, x = {}, y = {}, y_pred = {}, weight = {}-{}, '.format(s, last_input_data, inp_data, last_outputs, weight, po))

                sense_data = ord(state)
                data = list(map(ord, input_data))

                result = PROCESSOR[i].process(sense_data, data) 
                outputs, weight, po = result
                outputs = [[str(chr(x)).encode('utf-8') if i == 0 else x for x in outputs[i]] for i in range(len(input_data))] if len(outputs) > 0 else []
                    
                last_outputs = outputs
            last_input_data = [chr(x) if ix == 0 else x for ix, x in enumerate(data)]

            print()
if __name__ == '__main__':
    main()

